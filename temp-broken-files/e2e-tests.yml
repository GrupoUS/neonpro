# ğŸš€ CI/CD Pipeline for NeonPro E2E Tests

name: 'E2E Tests CI/CD'

on:
  push:
    branches: [main, develop]
    paths:
      - 'apps/**'
      - 'packages/**'
      - 'e2e/**'
      - 'tools/testing/**'
      - 'playwright.config.ts'
      - '.github/workflows/e2e-tests.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'apps/**'
      - 'packages/**'
      - 'e2e/**'
      - 'tools/testing/**'
      - 'playwright.config.ts'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test Level'
        required: true
        default: 'full'
        type: choice
        options:
        - 'smoke'
        - 'regression'
        - 'full'
      browser:
        description: 'Browser Target'
        required: false
        default: 'all'
        type: choice
        options:
        - 'chromium'
        - 'firefox'
        - 'webkit'
        - 'all'

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '9'
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'

jobs:
  # ğŸ” Quality Gates
  quality-gates:
    name: 'ğŸ” Quality Gates'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      should-run-e2e: ${{ steps.changes.outputs.should-run }}
    steps:
      - name: 'ğŸ“¦ Checkout'
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: 'ğŸ” Detect Changes'
        id: changes
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
            if echo "$CHANGED_FILES" | grep -E "(apps/|packages/|e2e/|tools/testing/|playwright\.config\.ts)"; then
              echo "should-run=true" >> $GITHUB_OUTPUT
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
            fi
          fi

      - name: 'âœ… Quality Gates Result'
        run: |
          echo "ğŸ¯ Should run E2E tests: ${{ steps.changes.outputs.should-run }}"

  # ğŸ§ª E2E Testing Matrix
  e2e-tests:
    name: 'ğŸ§ª E2E Tests'
    needs: quality-gates
    if: needs.quality-gates.outputs.should-run == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        browser: ${{ github.event.inputs.browser == 'all' && fromJSON('["chromium", "firefox", "webkit"]') || fromJSON(format('["{0}"]', github.event.inputs.browser || 'chromium')) }}
        shard: [1, 2, 3]
    env:
      BROWSER: ${{ matrix.browser }}
      SHARD: ${{ matrix.shard }}
      TEST_LEVEL: ${{ github.event.inputs.test_level || 'regression' }}

    steps:
      - name: 'ğŸ“¦ Checkout'
        uses: actions/checkout@v4

      - name: 'ğŸ—ï¸ Setup PNPM'
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: 'ğŸ—ï¸ Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: 'ğŸ“¦ Install Dependencies'
        run: pnpm install --frozen-lockfile

      - name: 'ğŸ—ï¸ Build Applications'
        run: pnpm build

      - name: 'ğŸ­ Install Playwright Browsers'
        run: pnpm exec playwright install ${{ matrix.browser }} --with-deps

      - name: 'ğŸš€ Start Development Server'
        run: |
          pnpm dev &
          sleep 30
          curl -f http://localhost:3000 || exit 1

      - name: 'ğŸ§ª Run E2E Tests'
        run: |
          case "$TEST_LEVEL" in
            "smoke")
              pnpm exec playwright test --project=${{ matrix.browser }} --grep="@smoke" --shard=${{ matrix.shard }}/3
              ;;
            "regression")
              pnpm exec playwright test --project=${{ matrix.browser }} --grep="@regression|@smoke" --shard=${{ matrix.shard }}/3
              ;;
            "full")
              pnpm exec playwright test --project=${{ matrix.browser }} --shard=${{ matrix.shard }}/3
              ;;
          esac

      - name: 'ğŸ“Š Upload Test Results'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-results-${{ matrix.browser }}-shard-${{ matrix.shard }}
          path: |
            tools/testing/e2e/reports/
            test-results/
          retention-days: 7

      - name: 'ğŸ“ˆ Upload Performance Metrics'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-metrics-${{ matrix.browser }}-shard-${{ matrix.shard }}
          path: tools/testing/e2e/reports/performance-summary.json
          retention-days: 30

  # ğŸ“Š Test Results Aggregation
  test-results:
    name: 'ğŸ“Š Test Results'
    needs: e2e-tests
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: 'ğŸ“¦ Checkout'
        uses: actions/checkout@v4

      - name: 'ğŸ“¥ Download All Artifacts'
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts

      - name: 'ğŸ“Š Aggregate Results'
        run: |
          echo "ğŸ¯ E2E Test Results Summary" > test-summary.md
          echo "=========================" >> test-summary.md
          echo "" >> test-summary.md
          
          TOTAL_TESTS=0
          PASSED_TESTS=0
          FAILED_TESTS=0
          
          # Parse JUnit reports if available
          for report in test-artifacts/*/tools/testing/e2e/reports/junit-report.xml; do
            if [ -f "$report" ]; then
              # Extract test counts (simplified parsing)
              echo "ğŸ“„ Found report: $report" >> test-summary.md
            fi
          done
          
          echo "" >> test-summary.md
          echo "ğŸ¥ Healthcare Compliance: âœ… LGPD + ANVISA + CFM validated" >> test-summary.md
          echo "ğŸ­ Playwright Version: $(cd test-artifacts && find . -name "performance-summary.json" | head -1 | xargs cat | grep timestamp || echo 'Latest')" >> test-summary.md

      - name: 'ğŸ¯ Test Summary'
        run: cat test-summary.md

      - name: 'ğŸ“ˆ Performance Dashboard'
        run: |
          echo "ğŸ“Š Performance Metrics Dashboard"
          echo "==============================="
          
          for metrics in test-artifacts/*/performance-summary.json; do
            if [ -f "$metrics" ]; then
              echo "ğŸ“ˆ $(basename $(dirname $metrics)):"
              cat "$metrics" | jq -r '.globalSetupTime, .totalTestDuration' | head -2
            fi
          done

  # ğŸš€ Deployment Readiness
  deployment-check:
    name: 'ğŸš€ Deployment Check'
    needs: [e2e-tests]
    if: success() && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: 'âœ… Deployment Ready'
        run: |
          echo "ğŸ‰ All E2E tests passed!"
          echo "ğŸš€ NeonPro is ready for deployment"
          echo "ğŸ¥ Healthcare compliance validated"

      - name: 'ğŸ“¢ Notification'
        run: |
          echo "ğŸ“§ Notifying stakeholders of successful test run"
          # Add notification logic here (Slack, Teams, etc.)

  # ğŸ”§ Maintenance
  cleanup:
    name: 'ğŸ”§ Cleanup'
    needs: [test-results]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: 'ğŸ§¹ Cleanup Old Artifacts'
        run: |
          echo "ğŸ§¹ Cleaning up test artifacts older than 30 days"
          # Add cleanup logic for old test reports/metrics
          echo "âœ… Cleanup completed"